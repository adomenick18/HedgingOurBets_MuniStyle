{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hedging our bets muni-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b33712bab031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhvplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "#import statements for packages/libraries\n",
    "#which libraries do we need?\n",
    "#pyfolio\n",
    "#scipy\n",
    "#backtester\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import hvplot.pandas\n",
    "from dotenv import load_dotenv\n",
    "import panel as pn\n",
    "from panel.interact import interact\n",
    "from panel import widgets\n",
    "import alpaca_trade_api as tradeapi\n",
    "import quandl\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in data from API (7-ish years of data 1/1/2013-11/01/2020)\n",
    "#bonds--bloomberg\n",
    "#muni etf (MUB)--alpaca\n",
    "#swap--bloomberg or alpaca\n",
    "#treasury--quandl\n",
    "#treasury ETF (TLT)--alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "\n",
    "api_key_QD = os.getenv(\"QUANDL_API_KEY\")\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "# type(api_key_QD)\n",
    "# type(alpaca_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import alpaca api file for ETF price history\n",
    "alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")\n",
    "\n",
    "# Format current date as ISO format\n",
    "start_date = pd.Timestamp(\"2013-01-02\", tz=\"America/New_York\").isoformat()\n",
    "today = pd.Timestamp(\"2020-11-02\", tz=\"America/New_York\").isoformat()\n",
    "\n",
    "# Set the tickers\n",
    "tickers = [\"MUB\", \"TLT\", \"UDN\", \"UUP\",\"LQD\",\"JNK\"]\n",
    "\n",
    "# Set timeframe to '1D' for Alpaca API\n",
    "timeframe = \"1D\"\n",
    "\n",
    "# Get closing prices\n",
    "df_ETFs = alpaca.get_barset(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = today\n",
    ").df\n",
    "\n",
    "# Preview DataFrame\n",
    "# YOUR CODE HERE!\n",
    "df_ETFs.head()\n",
    "\n",
    "# Output the data to CSV\n",
    "df_ETFs.to_csv(\"Resources/ETFs1.csv\", encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETF_csv = Path(\"Resources/ETFs.csv\")\n",
    "ETF_df = pd.read_csv(ETF_csv, index_col='Unnamed: 0', infer_datetime_format=True, parse_dates=True)\n",
    "#ETF_df.rename(columns={'Unnamed: 0': 'Date'}, inplace = True)\n",
    "ETF_df = ETF_df.drop(ETF_df.index[0])\n",
    "#ETF_df['Date'] = pd.to_datetime(ETF_df['Date'], utc=True)\n",
    "ETF_df.index = pd.to_datetime(ETF_df.index, utc=True).date\n",
    "#ETF_df['Date']= ETF_df['Date'].dt.strftime('%m/%d/%Y')\n",
    "ETF_df.sort_index(inplace=True)\n",
    "\n",
    "#drop unneccessary columns and keep close price columns\n",
    "ETF_close_prices_df = ETF_df.loc[:, ['JNK.3', 'LQD.3', 'MUB.3', 'TLT.3', 'UDN.3', 'UUP.3']]\n",
    "ETF_close_prices_df.rename(columns={'JNK.3': 'JNK',\n",
    "                   'LQD.3': 'LQD',\n",
    "                   'MUB.3': 'MUB',\n",
    "                   'TLT.3': 'TLT',\n",
    "                   'UDN.3': 'UDN',\n",
    "                   'UUP.3': 'UUP',\n",
    "                  }, inplace=True)\n",
    "\n",
    "ETF_close_prices_df[['JNK', 'LQD', 'MUB', 'TLT', 'UDN', 'UUP']] = ETF_close_prices_df[['JNK', 'LQD', 'MUB', 'TLT', 'UDN', 'UUP']].astype(float)\n",
    "ETF_close_prices_df.dtypes\n",
    "\n",
    "#calculate daily returns\n",
    "ETF_daily_returns_df = ETF_close_prices_df.pct_change().dropna()\n",
    "ETF_daily_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import quandl api file for US Treasury Rate history\n",
    "USTREASURY_data = quandl.get('USTREASURY/YIELD', start_date='2013-01-01', end_date='2020-11-06',authtoken=api_key_QD)\n",
    "#USTREASURY_data.to_csv('Resources/USTREASURY1.csv', encoding='utf-8', index=True)\n",
    "USTREASURY_data.head()\n",
    "columns_to_drop = ['1 MO', '2 MO', '3 MO', '6 MO']\n",
    "USTREASURY_data.drop(columns=columns_to_drop, inplace=True)\n",
    "USTREASURY_data.sort_index(inplace=True)\n",
    "\n",
    "#create new dataframe with maturity values\n",
    "columns = []\n",
    "tsy_mty_df = USTREASURY_data.copy()\n",
    "for column in USTREASURY_data.columns:\n",
    "    maturity = ''\n",
    "    for item in column.split():\n",
    "        if item.isdigit():\n",
    "            maturity = maturity + item\n",
    "    #print(int(maturity[0]))\n",
    "    tsy_mty_df[column] = int(maturity)\n",
    "    columns.append(maturity)\n",
    "USTREASURY_data.columns = columns\n",
    "tsy_mty_df.columns = columns\n",
    "\n",
    "#USTREASURY_data.head()\n",
    "#tsy_mty_df.head()\n",
    "\n",
    "#define function for calculating Treasury bond prices\n",
    "def bondprice(fv, c, ytm, t, m):\n",
    "    bondprice = ((fv*c/m*(1-(1+ytm/m)**(-m*t)))/(ytm/m)) + fv*(1+(ytm/m))**(-m*t)\n",
    "    return(bondprice)\n",
    "\n",
    "#bondprice(1000,0.06,0.08,9,2)\n",
    "#bondprice(100,0.0015,0.0014,1,2)\n",
    "\n",
    "#create new dataframe with treasury bond prices\n",
    "fv = 100\n",
    "c = USTREASURY_data.shift(1)/100\n",
    "ytm = USTREASURY_data/100\n",
    "m = 2\n",
    "t = tsy_mty_df\n",
    "USTREASURY_daily_prices = bondprice(fv, c, ytm, t, m)\n",
    "\n",
    "USTREASURY_data_daily_returns = USTREASURY_daily_prices.pct_change().dropna()\n",
    "#USTREASURY_data_daily_returns.head()\n",
    "USTREASURY_data_daily_returns.columns = USTREASURY_data_daily_returns.columns.astype(int)\n",
    "USTREASURY_data_daily_returns.head()\n",
    "#tsy_mty_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in LIBOR swaps to create pct_change df\n",
    "## the sort_index for dates doesn't seem to be working...\n",
    "swaps_csv = Path(\"Resources/libor_swaps.csv\")\n",
    "swaps_df = pd.read_csv(swaps_csv, index_col='Date', infer_datetime_format=True, parse_dates=True)\n",
    "#swaps_df.set_index('Date',inplace=True)\n",
    "swaps_df.sort_index(inplace=True)\n",
    "\n",
    "swaps_mty_df = swaps_df.copy()\n",
    "for column in swaps_df.columns:\n",
    "    swaps_mty_df[column] = int(column)\n",
    "\n",
    "#create new dataframe with libor swap value (using bond price function)\n",
    "fv = 100\n",
    "c = swaps_df.shift(1)/100\n",
    "ytm = swaps_df/100\n",
    "m = 2\n",
    "t = swaps_mty_df\n",
    "swaps_df_values = bondprice(fv, c, ytm, t, m)\n",
    "\n",
    "swaps_daily_returns = swaps_df_values.pct_change().dropna()\n",
    "swaps_daily_returns.columns = swaps_daily_returns.columns.astype(int)\n",
    "swaps_daily_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in bond prices and create dataframe\n",
    "bonds_csv = Path(\"Resources/bonds_example_prices_larger.csv\")\n",
    "bonds_df = pd.read_csv(bonds_csv, index_col='Date',infer_datetime_format=True, parse_dates=True)\n",
    "bonds_df.sort_index(inplace=True)\n",
    "\n",
    "#need to wait to dropna after concat within the correlation function\n",
    "bonds_daily_returns = bonds_df.pct_change()\n",
    "bonds_daily_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in bond descriptions to dataframe\n",
    "bondescr_csv = Path(\"Resources/muni_hedge_characteristics.csv\")\n",
    "bondescr_df = pd.read_csv(bondescr_csv)\n",
    "\n",
    "cleaned_cusips = []\n",
    "for cusipid in bondescr_df['CUSIP']:\n",
    "    cusipid = cusipid.strip()\n",
    "    cleaned_cusips.append(cusipid)\n",
    "\n",
    "bondescr_df['CUSIP'] = cleaned_cusips\n",
    "\n",
    "bondescr_df.set_index('CUSIP',inplace=True)\n",
    "bondescr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function passing a cusip to put it against hedges\n",
    "#what are the steps we need our function to do:\n",
    "    #1.pick bond\n",
    "    #2. compare versus all hedges\n",
    "    #3. show correlation of bond vis a vis different hedges\n",
    "#ETF_daily_returns_df, USTREASURY_data_daily_returns, swaps_daily_returns, bonds_daily_returns  \n",
    "#cusip = '57583R5Q4'\n",
    "def historical_corr(cusip):\n",
    "    # find maturity of chosen muni bond and convert to datetime\n",
    "    bond_mty = bondescr_df.loc[cusip,'Maturity Date']\n",
    "    bond_mty = dt.datetime.strptime(bond_mty, '%m/%d/%Y')\n",
    "    bond_mty\n",
    "\n",
    "    # create series with bond years to maturity for lookup in dataframes\n",
    "    # relativedelta function: https://dateutil.readthedocs.io/en/stable/relativedelta.html\n",
    "    ## make output for years more exact by looking at days/360 and rounding?\n",
    "    mty_years = []\n",
    "    for return_date in bonds_daily_returns.index:\n",
    "        difference = relativedelta(bond_mty, return_date)\n",
    "        mty_years.append(difference.years)\n",
    "    #len(mty_years)\n",
    "\n",
    "    # reference closest maturity in tsy and swap df\n",
    "    # USTREASURY_data_daily_returns, swaps_daily_returns, mty_years\n",
    "    tsy_index = []\n",
    "    for item in mty_years:\n",
    "        min_item = ' '\n",
    "        min_value = 0\n",
    "        for col in USTREASURY_data_daily_returns.columns:\n",
    "            if min_item == ' ':\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "            elif abs(col - int(item)) < min_value:\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "        tsy_index.append(min_item)\n",
    "\n",
    "    swap_index = []\n",
    "    for item in mty_years:\n",
    "        min_item = ' '\n",
    "        min_value = 0\n",
    "        for col in swaps_daily_returns.columns:\n",
    "            if min_item == ' ':\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "            elif abs(col - int(item)) < min_value:\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "        swap_index.append(min_item)\n",
    "    #len(swap_index)\n",
    "    #len(tsy_index)\n",
    "\n",
    "    # create new dataframe to combine items\n",
    "    # USTREASURY_data_daily_returns, swaps_daily_returns, mty_years\n",
    "    joined_df = pd.DataFrame(bonds_daily_returns[cusip])\n",
    "    joined_df['tsy_index'] = tsy_index\n",
    "    joined_df['swap_index'] = swap_index\n",
    "    \n",
    "    joined_df = pd.merge(joined_df, USTREASURY_data_daily_returns, left_index=True, right_index=True)\n",
    "\n",
    "    tsy_change = []\n",
    "    for indx in joined_df.index:\n",
    "        change = joined_df.loc[indx, joined_df['tsy_index'][indx]]\n",
    "        tsy_change.append(change)\n",
    "    joined_df['tsy_change'] = tsy_change\n",
    "    tsy_col_drop = ['tsy_index',1,2,3,5,7,10,20,30]\n",
    "    joined_df.drop(columns=tsy_col_drop,inplace=True)\n",
    "\n",
    "    joined_df = pd.merge(joined_df, swaps_daily_returns, left_index=True, right_index=True)\n",
    "\n",
    "    swap_change = []\n",
    "    for indx in joined_df.index:\n",
    "        change = joined_df.loc[indx, joined_df['swap_index'][indx]]\n",
    "        swap_change.append(change)\n",
    "    joined_df['swap_change'] = swap_change\n",
    "    swap_col_drop = ['swap_index',1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "    joined_df.drop(columns=swap_col_drop,inplace=True)\n",
    "\n",
    "    ##concat separate dataframes (muni etf, treasury etf)\n",
    "    #ETF_daily_returns_df\n",
    "    joined_df = pd.merge(joined_df, ETF_daily_returns_df, left_index=True, right_index=True)\n",
    "    #joined_df.head()\n",
    "\n",
    "    ## calculate standard deviation\n",
    "    hedge_corr = joined_df.corr()\n",
    "    return hedge_corr\n",
    "hedge_corr = historical_corr(cusip)\n",
    "hedge_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedge_corr.style.background_gradient(cmap='YlGn_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(hedge_corr, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_corr_test(cusip):\n",
    "    # find maturity of chosen muni bond and convert to datetime\n",
    "    bond_mty = bondescr_df.loc[cusip,'Maturity Date']\n",
    "    bond_mty = dt.datetime.strptime(bond_mty, '%m/%d/%Y')\n",
    "    bond_mty\n",
    "\n",
    "    # create series with bond years to maturity for lookup in dataframes\n",
    "    # relativedelta function: https://dateutil.readthedocs.io/en/stable/relativedelta.html\n",
    "    ## make output for years more exact by looking at days/360 and rounding?\n",
    "    mty_years = []\n",
    "    for return_date in bonds_daily_returns.index:\n",
    "        difference = relativedelta(bond_mty, return_date)\n",
    "        mty_years.append(difference.years)\n",
    "    #len(mty_years)\n",
    "\n",
    "    # reference closest maturity in tsy and swap df\n",
    "    # USTREASURY_data_daily_returns, swaps_daily_returns, mty_years\n",
    "    tsy_index = []\n",
    "    for item in mty_years:\n",
    "        min_item = ' '\n",
    "        min_value = 0\n",
    "        for col in USTREASURY_data_daily_returns.columns:\n",
    "            if min_item == ' ':\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "            elif abs(col - int(item)) < min_value:\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "        tsy_index.append(min_item)\n",
    "\n",
    "    swap_index = []\n",
    "    for item in mty_years:\n",
    "        min_item = ' '\n",
    "        min_value = 0\n",
    "        for col in swaps_daily_returns.columns:\n",
    "            if min_item == ' ':\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "            elif abs(col - int(item)) < min_value:\n",
    "                min_item = col\n",
    "                min_value = abs(col - int(item))\n",
    "        swap_index.append(min_item)\n",
    "    #len(swap_index)\n",
    "    #len(tsy_index)\n",
    "\n",
    "    # create new dataframe to combine items\n",
    "    # USTREASURY_data_daily_returns, swaps_daily_returns, mty_years\n",
    "    joined_df = pd.DataFrame(bonds_daily_returns[cusip])\n",
    "    joined_df['tsy_index'] = tsy_index\n",
    "    joined_df['swap_index'] = swap_index\n",
    "    \n",
    "    joined_df = pd.merge(joined_df, USTREASURY_data_daily_returns, left_index=True, right_index=True)\n",
    "\n",
    "    tsy_change = []\n",
    "    for indx in joined_df.index:\n",
    "        change = joined_df.loc[indx, joined_df['tsy_index'][indx]]\n",
    "        tsy_change.append(change)\n",
    "    joined_df['tsy_change'] = tsy_change\n",
    "    tsy_col_drop = ['tsy_index',1,2,3,5,7,10,20,30]\n",
    "    joined_df.drop(columns=tsy_col_drop,inplace=True)\n",
    "\n",
    "    joined_df = pd.merge(joined_df, swaps_daily_returns, left_index=True, right_index=True)\n",
    "\n",
    "    swap_change = []\n",
    "    for indx in joined_df.index:\n",
    "        change = joined_df.loc[indx, joined_df['swap_index'][indx]]\n",
    "        swap_change.append(change)\n",
    "    joined_df['swap_change'] = swap_change\n",
    "    swap_col_drop = ['swap_index',1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "    joined_df.drop(columns=swap_col_drop,inplace=True)\n",
    "\n",
    "    ##concat separate dataframes (muni etf, treasury etf)\n",
    "    #ETF_daily_returns_df\n",
    "    joined_df = pd.merge(joined_df, ETF_daily_returns_df, left_index=True, right_index=True)\n",
    "    #joined_df.head()\n",
    "\n",
    "    ## calculate standard deviation\n",
    "    hedge_corr = joined_df.corr()\n",
    "    return hedge_corr.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bonds_daily_returns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d301d05b3ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtsy_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbonds_daily_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcusip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtsy_corr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_corr_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcusip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtsy_corr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bonds_daily_returns' is not defined"
     ]
    }
   ],
   "source": [
    "tsy_corr = []\n",
    "for item in bonds_daily_returns.columns:\n",
    "    cusip = item\n",
    "    tsy_corr.append(historical_corr_test(cusip))\n",
    "tsy_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
